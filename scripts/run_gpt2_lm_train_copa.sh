# generation
# python ./src/method1/run_gpt2_lm_train_copa.py --model_name_or_path=/home/niuyilin/pre-trained-models/gpt2-large --output_dir=./runs/GPT2-large-LM-trainONmnli-bs8/checkpoint-12000 --model_type=gpt2 --train_data_file=./data/COPA/COPA-resources/datasets/copa-dev.xml --eval_data_file=./data/COPA/COPA-resources/datasets/copa-test.xml --do_generate --repetition_penalty=1
# python ./src/method1/run_gpt2_lm_train_copa.py --model_name_or_path=/home/niuyilin/pre-trained-models/gpt2-large --output_dir=./runs/GPT2-large-LM-trainONmnli-bs8/checkpoint-12000 --model_type=gpt2 --train_data_file=./data/COPA/COPA-resources/datasets/copa-dev.xml --eval_data_file=./data/COPA/COPA-resources/datasets/copa-test.xml.posGeneration.Top1 --do_generate --repetition_penalty=1
python ./src/method1/run_gpt2_lm_train_copa.py --model_name_or_path=/home/niuyilin/pre-trained-models/gpt2-large --output_dir=./runs/GPT2-large-LM-trainONmnli-bs8/checkpoint-12000 --model_type=gpt2 --train_data_file=./data/COPA/COPA-resources/datasets/copa-dev.xml --eval_data_file=./data/COPA/COPA-resources/datasets/copa-test.xml.bothGeneration.Top1 --do_generate --repetition_penalty=1

# train (mask the one entire premise sentence): loss = -P(pos)
# python ./src/method1/run_gpt2_lm_train_copa.py --model_name_or_path=/home/niuyilin/pre-trained-models/gpt2-large --output_dir=./runs/GPT2-large-LM-trainONcopa --num_train_epochs=4 --learning_rate=1e-5 --model_type=gpt2 --do_train --train_data_file=./data/COPA/COPA-resources/datasets/copa-dev.xml --do_eval --eval_data_file=./data/COPA/COPA-resources/datasets/copa-test.xml --per_gpu_train_batch_size=16 --per_gpu_eval_batch_size=16 --gradient_accumulation_steps=1 --evaluate_during_training --save_steps=30 --logging_steps=10